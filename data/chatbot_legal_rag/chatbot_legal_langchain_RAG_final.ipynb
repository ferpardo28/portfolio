{"cells":[{"cell_type":"markdown","metadata":{"id":"-hVND8xY2OKY"},"source":["# Chatbot legal con RAG y LangChain\n","\n","Este proyecto explora cómo usar modelos de lenguaje (LLMs) junto con recuperación aumentada (RAG) para responder preguntas legales utilizando leyes mexicanas como base de conocimiento."]},{"cell_type":"markdown","metadata":{"id":"xpPmj694iRKm"},"source":["## Introducción\n","\n","Las leyes mexicanas, como la Ley General de Protección de Datos Personales o la Ley de Transparencia, contienen información extensa y técnica. Este proyecto presenta un chatbot legal que utiliza técnicas de Recuperación Aumentada (RAG) con LangChain para ayudar a los usuarios a consultar contenido legal de manera eficiente.\n","\n","A través del procesamiento de texto, creación de vectores semánticos y una interfaz conversacional, el sistema puede ofrecer respuestas informadas usando documentos legales como fuente primaria."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"9AyE4xFfS3JS","executionInfo":{"status":"ok","timestamp":1751582843258,"user_tz":360,"elapsed":18992,"user":{"displayName":"fernando pardo","userId":"01463800194653286324"}}},"outputs":[],"source":["!pip install langchain langchain-community langchain-openai faiss-cpu pdfplumber > /dev/null 2>&1\n","!pip install pypdf > /dev/null 2>&1\n","!pip install gradio > /dev/null 2>&1\n","!wget --no-check-certificate -O LFT.pdf https://www.diputados.gob.mx/LeyesBiblio/pdf/LFT.pdf > /dev/null 2>&1"]},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2025-06-19T01:09:24.972419Z","start_time":"2025-06-19T01:08:40.956352Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Bfs5Zxc9j7Uf","outputId":"3aa23028-a14c-41f4-810b-50cc080da85c","executionInfo":{"status":"ok","timestamp":1751582632078,"user_tz":360,"elapsed":42772,"user":{"displayName":"fernando pardo","userId":"01463800194653286324"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando y dividiendo el documento: LFT.pdf\n","Número de fragmentos: 1834\n","Creando o cargando la base de datos vectorial...\n","Base de datos vectorial cargada.\n","Preparación de la base de datos vectorial completada.\n"]}],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import PyPDFLoader\n","import os\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","from typing import List, Tuple\n","import gradio as gr\n","from openai import OpenAI\n","\n","\n","OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n","\n","embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n","\n","db_path = \"faiss_index\"\n","def load_and_split_pdf(file_path):\n","    \"\"\"Carga un archivo PDF y lo divide en fragmentos manejables.\"\"\"\n","    loader = PyPDFLoader(file_path)\n","    documents = loader.load()\n","\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,  # Tamaño de cada fragmento\n","        chunk_overlap=200 # Superposición entre fragmentos para mantener el contexto\n","    )\n","    chunks = text_splitter.split_documents(documents)\n","    return chunks\n","\n","# Generar Embeddings y crear la base de datos vectorial\n","def create_vector_store(chunks):\n","    \"\"\"\n","    Función `create_vector_store` documentada automáticamente para el portafolio.\n","    \"\"\"\n","    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n","\n","    # Crear la base de datos vectorial a partir de los chunks y embeddings\n","    # Si ya existe, podemos cargarlo en lugar de crearlo de nuevo\n","    if os.path.exists(db_path):\n","        vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)\n","        print(\"Base de datos vectorial cargada.\")\n","    else:\n","        vector_store = FAISS.from_documents(chunks, embeddings)\n","        vector_store.save_local(db_path)\n","        print(\"Base de datos vectorial creada y guardada.\")\n","    return vector_store\n","\n","\n","pdf_path = \"LFT.pdf\"\n","\n","print(f\"Cargando y dividiendo el documento: {pdf_path}\")\n","text_chunks = load_and_split_pdf(pdf_path)\n","print(f\"Número de fragmentos: {len(text_chunks)}\")\n","\n","print(\"Creando o cargando la base de datos vectorial...\")\n","vector_db = create_vector_store(text_chunks)\n","print(\"Preparación de la base de datos vectorial completada.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2025-06-19T01:09:25.035022Z","start_time":"2025-06-19T01:09:25.023052Z"},"id":"d2czopuMvufg","executionInfo":{"status":"ok","timestamp":1751582632079,"user_tz":360,"elapsed":3,"user":{"displayName":"fernando pardo","userId":"01463800194653286324"}}},"outputs":[],"source":["def load_vector_store():\n","    \"\"\"Carga la base de datos vectorial desde el disco\"\"\"\n","    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n","    vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)\n","    return vector_store\n","\n","def setup_rag_chain(vector_store):\n","    \"\"\"Configura la cadena RAG con el LLM y la base de datos vectorial\"\"\"\n","    llm = ChatOpenAI(temperature=0.1, api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n","\n","    # Custom prompt para guiar al LLM\n","    template = \"\"\"Eres un asistente de IA experto en la Ley Federal del Trabajo de México.\n","    Tu objetivo es proporcionar respuestas precisas y claras basadas exclusivamente en el contexto proporcionado.\n","    Si la pregunta no puede ser respondida con la información del contexto, responde que no tienes información suficiente.\n","    Responde en lenguaje claro y sencillo, como si explicaras a una persona sin conocimientos legales.\n","\n","    Contexto: {context}\n","    Pregunta: {question}\n","    Respuesta:\"\"\"\n","\n","    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n","\n","    # Crear la cadena de recuperación y respuesta\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm=llm,\n","        chain_type=\"stuff\", # Combina todos los documentos recuperados en un solo prompt\n","        retriever=vector_store.as_retriever(),\n","        return_source_documents=True, # Para ver de dónde proviene la información\n","        chain_type_kwargs={\"prompt\": prompt}\n","    )\n","    return qa_chain\n","\n","# Simulación del chatbot\n","\n","def chat_with_bot(user_query):\n","    \"\"\"Función principal para interactuar con el chatbot\"\"\"\n","    # print(\"Cargando base de datos vectorial...\")\n","    # vector_db = load_vector_store()\n","    # print(\"Base de datos vectorial cargada. Iniciando chatbot...\")\n","    qa_system = setup_rag_chain(vector_db)\n","\n","    print(\"\\n¡Hola! Soy tu asistente sobre la Ley Federal del Trabajo de México. ¿En qué puedo ayudarte? (Escribe 'salir' para terminar)\")\n","\n","\n","    print(f\"Procesando tu pregunta: {user_query}\")\n","    if user_query.lower() == 'salir':\n","        print(\"¡Hasta luego!\")\n","    else:\n","        try:\n","            result = qa_system.invoke({\"query\": user_query})\n","            print(\"\\nRespuesta del Bot:\")\n","            print(result[\"result\"])\n","\n","            # Para debuggear las fuentes\n","            # print(\"\\nDocumentos Fuente:\")\n","            # for doc in result[\"source_documents\"]:\n","            #     print(f\"- Contenido: {doc.page_content[:200]}...\")\n","            #     print(f\"  Metadata: {doc.metadata}\")\n","\n","        except Exception as e:\n","            print(f\"Ha ocurrido un error: {e}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"hqzUgzOqYrW7","executionInfo":{"status":"ok","timestamp":1751582632084,"user_tz":360,"elapsed":4,"user":{"displayName":"fernando pardo","userId":"01463800194653286324"}}},"outputs":[],"source":["# TODO: Cambiar a formato {\"role\": ..., \"content\": ...} para Gradio\n","\n","\n","#variable global para almacenar el historial\n","conversation_history = []\n","\n","def chat_with_bot(message: str, history=None):\n","    \"\"\"Versión simplificada con memoria básica\"\"\"\n","    global conversation_history\n","\n","    qa_system = setup_rag_chain(vector_db)\n","\n","    if message.lower() == 'salir':\n","        conversation_history = []  # Limpiar historial al salir\n","        return \"¡Hasta luego!\"\n","\n","    if message.lower() == 'limpiar':\n","        conversation_history = []\n","        return \"Historial de conversación borrado.\"\n","\n","    try:\n","        # Añadir contexto de las últimas 3 preguntas/respuestas\n","        context = \"\\n\".join([f\"Pregunta: {q}\\nRespuesta: {r}\" for q, r in conversation_history[-3:]])\n","\n","        # Crear pregunta con contexto\n","        full_query = f\"Contexto previo:\\n{context}\\n\\nNueva pregunta: {message}\"\n","\n","        result = qa_system({\"query\": full_query})\n","        response = result[\"result\"]\n","\n","        # Guardar en el historial (máximo 5 intercambios)\n","        conversation_history.append((message, response))\n","        if len(conversation_history) > 5:\n","            conversation_history.pop(0)\n","\n","        return response\n","\n","    except Exception as e:\n","        return f\"Error: {str(e)}\""]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"id":"LqTI2m9XYvhx","outputId":"cf7cfe6c-4631-4f61-c93d-c56d38679233","executionInfo":{"status":"ok","timestamp":1751582634643,"user_tz":360,"elapsed":2558,"user":{"displayName":"fernando pardo","userId":"01463800194653286324"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://03957376eed7fd8f9f.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://03957376eed7fd8f9f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}],"source":["def launch_chat_interface():\n","    \"\"\"Lanza la interfaz interactiva del chatbot con disclaimer legal\"\"\"\n","    # Ejemplos de preguntas para mostrar en la interfaz\n","    examples = [\n","        \"¿En qué casos un trabajador puede ser despedido sin responsabilidad para el patrón?\",\n","        \"¿Cuáles son las prestaciones de ley?\",\n","        \"Si me ausento por 15 días sin avisar, ¿qué pasa?\"\n","    ]\n","\n","    # Configuración de la interfaz con disclaimer legal\n","    disclaimer = \"\"\"\n","    ⚠️ **AVISO LEGAL IMPORTANTE** ⚠️\\n\n","    Este asistente virtual proporciona información general sobre la Ley Federal del Trabajo de México,\n","    pero **NO sustituye el asesoramiento jurídico profesional**.\\n\n","    • No soy un abogado licenciado\\n\n","    • Mis respuestas son generadas automáticamente basadas en el texto de la ley\\n\n","    • Para casos específicos o asesoría legal personalizada, consulta con un profesional del derecho\\n\n","    • El Tecnológico de Monterrey no se hace responsable por el uso que se dé a esta información\\n\n","    \\n\n","    ¿En qué puedo ayudarte hoy respecto a la Ley Federal del Trabajo?\n","    \"\"\"\n","\n","    demo = gr.ChatInterface(\n","        fn=chat_with_bot,\n","        title=\"Asistente Informativo de la LFT\",\n","        description=disclaimer,\n","        examples=examples,\n","        theme=\"monochrome\"\n","    )\n","\n","    # Lanzar la interfaz\n","    demo.launch(share=True)\n","\n","# Ejecuta esta función para probar la interfaz\n","launch_chat_interface()"]},{"cell_type":"markdown","source":["## Interfaz Gradio\n","\n","> Al ejecutar la app, Gradio generará un enlace público (ej. `https://xxxx.gradio.live`)  \n","> que permite probar el chatbot en el navegador.  \n","> Este enlace es temporal y solo funciona mientras el Colab está activo.\n"],"metadata":{"id":"ipOHqyPrgEYQ"}},{"cell_type":"markdown","metadata":{"id":"3w3usdaC0BCj"},"source":["## Conclusión\n","\n","Este proyecto demuestra cómo combinar modelos de lenguaje (LLMs) con técnicas de recuperación aumentada (RAG) para construir chatbots que pueden responder preguntas complejas basadas en documentos legales. Se implementó un flujo completo usando LangChain, FAISS y una interfaz conversacional, permitiendo consultas útiles sobre leyes mexicanas. Este enfoque puede extenderse a otros dominios como finanzas, medicina o recursos humanos."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}